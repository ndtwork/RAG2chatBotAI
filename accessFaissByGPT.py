import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer

# ğŸ“¥ 1. Táº£i FAISS index
index = faiss.read_index("vector_index.faiss")

# ğŸ“¥ 2. Táº£i metadata (Ä‘á»ƒ truy xuáº¥t Ä‘oáº¡n vÄƒn gá»‘c)
with open("chunk_metadata.pkl", "rb") as f:
    paragraphs = pickle.load(f)

# ğŸš€ 3. Khá»Ÿi táº¡o mÃ´ hÃ¬nh nhÃºng
embedding_model = SentenceTransformer("keepitreal/vietnamese-sbert")


def preprocess_text(text):
    """Chuáº©n hÃ³a vÄƒn báº£n trÆ°á»›c khi nhÃºng"""
    return text.lower().strip()


def retrieve_relevant_chunks(question, index, paragraphs, embedding_model, top_k=3):
    """TÃ¬m cÃ¡c Ä‘oáº¡n vÄƒn báº£n liÃªn quan tá»« FAISS"""
    # ğŸ”¹ 1. Chuáº©n hÃ³a & mÃ£ hÃ³a cÃ¢u há»i
    question_vector = embedding_model.encode([preprocess_text(question)])

    # ğŸ”¹ 2. Chuáº©n hÃ³a vector Ä‘á»ƒ dÃ¹ng Cosine Similarity
    faiss.normalize_L2(question_vector)

    # ğŸ”¹ 3. TÃ¬m top_k vector gáº§n nháº¥t trong FAISS
    distances, indices = index.search(question_vector, top_k)

    # ğŸ”¹ 4. Láº¥y cÃ¡c Ä‘oáº¡n vÄƒn báº£n tÆ°Æ¡ng á»©ng
    relevant_chunks = [paragraphs[idx] for idx in indices[0] if idx != -1]

    return relevant_chunks


# ğŸ” VÃ­ dá»¥ cÃ¢u há»i
question = "Quy Ä‘á»‹nh vá» Ä‘iá»ƒm thi tháº¿ nÃ o?"

# ğŸ”¥ Gá»i hÃ m truy xuáº¥t
relevant_chunks = retrieve_relevant_chunks(question, index, paragraphs, embedding_model)

# ğŸ“ In káº¿t quáº£
print("CÃ¡c Ä‘oáº¡n vÄƒn báº£n liÃªn quan:")
for i, chunk in enumerate(relevant_chunks, 1):
    print(f"{i}. {chunk}")
